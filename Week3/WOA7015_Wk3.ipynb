{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WOA7015_Wk3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIgFUm0FAZOu775EIoFBzm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiernee/Advanced_ML/blob/main/Week3/WOA7015_Wk3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzY5ZsYGoDdu"
      },
      "source": [
        "# Welcome to WOA7015 Advance Machine Learning Lab - Week 3\n",
        "This code is generated for the purpose of WOA7015 module.\n",
        "The code is available in github https://github.com/shiernee/Advanced_ML \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVerYxHW-ZrQ"
      },
      "source": [
        "# The effect of imbalanced data on AUROC \n",
        "The following code evaluates the effect of imbalanced data on the AUROC of TPR-FPR curve. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c87yzg0goBrP"
      },
      "source": [
        "# roc curve and auc on an imbalanced dataset\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.under_sampling import RandomUnderSampler\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcpJntDPEq2J",
        "outputId": "6e6cbb8c-8ee1-43a6-91bc-64bdfad9412f"
      },
      "source": [
        "# generate 2 class dataset \n",
        "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1000)\n",
        "\n",
        "print(X)\n",
        "print('-----------')\n",
        "print(y)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.32584935  0.21897754  0.62061895 ...  2.84071377 -0.02582733\n",
            "  -0.40885762]\n",
            " [-1.12624124 -0.86026727 -0.89264356 ... -0.92962064  0.59483549\n",
            "   1.24052468]\n",
            " [-0.48993428 -0.7453348  -1.43801838 ... -1.67525801 -0.09994425\n",
            "  -0.46569289]\n",
            " ...\n",
            " [ 0.47406074 -1.9209351   0.41681779 ...  1.04574815  1.092832\n",
            "  -0.01541749]\n",
            " [-0.62731673 -0.94336697 -1.50694171 ... -0.85092941  0.99046917\n",
            "   2.19583454]\n",
            " [ 0.88990126  0.81857103 -2.12551556 ...  1.00271323 -0.88101446\n",
            "  -0.81149645]]\n",
            "-----------\n",
            "[1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0\n",
            " 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0\n",
            " 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0\n",
            " 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1\n",
            " 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0\n",
            " 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLo12OKVE__J",
        "outputId": "a6dbfbe9-fecd-4d53-b281-c7193c79caa2"
      },
      "source": [
        "# split into train/test sets\n",
        "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=1000)\n",
        "\n",
        "print('trainy - class0: ', len(trainy)-trainy.sum())\n",
        "print('trainy - class1: ', trainy.sum())\n",
        "print('----------------------')\n",
        "print('testy - class0: ', len(testy)-testy.sum())\n",
        "print('testy - class1: ', testy.sum())\n",
        "print('============================')\n",
        "\n",
        "# make testing dataset balance\n",
        "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
        "testX, testy = undersample.fit_resample(testX, testy)\n",
        "\n",
        "print('Balanced Testing date')\n",
        "print('testy - class0: ', len(testy)-testy.sum())\n",
        "print('testy - class1: ', testy.sum())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainy - class0:  253\n",
            "trainy - class1:  247\n",
            "----------------------\n",
            "testy - class0:  249\n",
            "testy - class1:  251\n",
            "============================\n",
            "Balanced Testing date\n",
            "testy - class0:  249\n",
            "testy - class1:  249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3QmySaLE7Nm"
      },
      "source": [
        "# fit a model with training data\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "model.fit(trainX, trainy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOu_783uGpZd"
      },
      "source": [
        "# repeat with different skewness \n",
        "roc_list = []\n",
        "lr_acc = []\n",
        "k=1\n",
        "for i in range(0, 10):\n",
        "  pos_ind = np.where(testy==1)[0]\n",
        "  n = int(i/10 * len(pos_ind))\n",
        "  tmp_testX, tmp_testy = np.copy(testX), np.copy(testy)\n",
        "  tmp_testX = np.delete(tmp_testX, pos_ind[:n], axis=0)\n",
        "  tmp_testy = np.delete(tmp_testy, pos_ind[:n], axis=0)\n",
        "  print('nth %d:positive: %d negative: %d' \n",
        "        % (i, tmp_testy.sum(), tmp_testy.shape[0] - tmp_testy.sum()))\n",
        "  print('---------------------------------------------')\n",
        "  \n",
        "  # predict probabilities\n",
        "  lr_probs = model.predict_proba(tmp_testX)\n",
        "  # keep probabilities for the positive outcome only\n",
        "  lr_probs = lr_probs[:, 1]\n",
        "  # calculate scores\n",
        "  lr_auc = roc_auc_score(tmp_testy, lr_probs)\n",
        "\n",
        "  # summarize scores\n",
        "  # print('iteration %d: Logistic: ROC AUC=%.3f' % (k, lr_auc))\n",
        "  k += 1\n",
        "  # calculate roc curves\n",
        "  lr_fpr, lr_tpr, _ = roc_curve(tmp_testy, lr_probs)\n",
        "  roc_list.append(lr_auc)\n",
        "\n",
        "plt.plot(np.arange(0, len(roc_list)), roc_list)\n",
        "plt.xlabel('skewness ratio')\n",
        "plt.ylabel('AUROC')\n",
        "plt.title('decreasing positive sample')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njFP2GHpC1VE"
      },
      "source": [
        "# Exercise 1 (2%):\n",
        "Does the AUROC (TPR vs FPR) affected by imbalanced class?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOIpcKliC56h"
      },
      "source": [
        "# Your answer here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4Cxp0q75PM"
      },
      "source": [
        "# The effect of imbalanced data on AUROC of PR curve and F1 score\n",
        "The following code evaluates the effect of imbalanced data on the AUROC of Precision-Recall and F1 value. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYxjJuD_8ewJ"
      },
      "source": [
        "# roc curve and auc on an imbalanced dataset\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import auc, f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr_uY_mPHLTF"
      },
      "source": [
        "# generate 2 class dataset \n",
        "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1000)\n",
        "\n",
        "print(X)\n",
        "print('-----------')\n",
        "print(y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbmv6pFhHWyQ"
      },
      "source": [
        "# split into train/test sets\n",
        "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=1000)\n",
        "\n",
        "print('trainy - class0: ', len(trainy)-trainy.sum())\n",
        "print('trainy - class1: ', trainy.sum())\n",
        "print('----------------------')\n",
        "print('testy - class0: ', len(testy)-testy.sum())\n",
        "print('testy - class1: ', testy.sum())\n",
        "print('============================')\n",
        "\n",
        "# make testing dataset balance\n",
        "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
        "testX, testy = undersample.fit_resample(testX, testy)\n",
        "\n",
        "print('Balanced Testing date')\n",
        "print('testy - class0: ', len(testy)-testy.sum())\n",
        "print('testy - class1: ', testy.sum())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1ST-YEcHh5I"
      },
      "source": [
        "# fit a model\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "model.fit(trainX, trainy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scPocodDHRp-"
      },
      "source": [
        "# repeat with different skewness \n",
        "roc_list = []\n",
        "f1_list = []\n",
        "\n",
        "k=1\n",
        "for i in range(0, 10):\n",
        "  pos_ind = np.where(testy==1)[0]\n",
        "  n = int(i/10 * len(pos_ind))\n",
        "  tmp_testX, tmp_testy = np.copy(testX), np.copy(testy)\n",
        "  tmp_testX = np.delete(tmp_testX, pos_ind[:n], axis=0)\n",
        "  tmp_testy = np.delete(tmp_testy, pos_ind[:n], axis=0)\n",
        "  print('nth %d:positive: %d negative: %d' \n",
        "        % (i, tmp_testy.sum(), tmp_testy.shape[0] - tmp_testy.sum()))\n",
        "  print('---------------------------------------------')\n",
        "  \n",
        "\n",
        "  # predict probabilities\n",
        "  lr_probs = model.predict_proba(tmp_testX)\n",
        "  # keep probabilities for the positive outcome only\n",
        "  lr_probs = lr_probs[:, 1]\n",
        "  # predict class values\n",
        "  yhat = model.predict(tmp_testX)\n",
        "  # calculate precision and recall for each threshold\n",
        "  lr_precision, lr_recall, _ = precision_recall_curve(tmp_testy, lr_probs)\n",
        "  # calculate scores\n",
        "  lr_f1, lr_auc = f1_score(tmp_testy, yhat), auc(lr_recall, lr_precision)\n",
        "  # summarize scores\n",
        "  # print('iteration%d Logistic: f1=%.3f auc=%.3f' % (k, lr_f1, lr_auc))\n",
        "  k += 1\n",
        "  roc_list.append(lr_auc)\n",
        "  f1_list.append(lr_f1)\n",
        "\n",
        "plt.plot(np.arange(0, len(roc_list)), roc_list)\n",
        "plt.xlabel('skewness ratio')\n",
        "plt.ylabel('AUC of PR curve')\n",
        "plt.title('decreasing positive sample')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, len(roc_list)), f1_list)\n",
        "plt.xlabel('skewness ratio')\n",
        "plt.ylabel('F1')\n",
        "plt.title('decreasing positive sample')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DP3LOZY7v6M"
      },
      "source": [
        "# Exercise 2 (4%):\n",
        "Does the AUROC (Precision vs Recall), F1 score affected by imbalanced class?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPXGu2E00-c5"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h1qUoy4DSc2"
      },
      "source": [
        "# ***Let's go back to power point - slide 13***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK3GJxwgzEjP"
      },
      "source": [
        "# Convex function\n",
        "\n",
        "This is the code to generate the graph in slide 38"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v0il6m3zOQb"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "\n",
        "x = np.arange(-2, 2, 0.01)\n",
        "\n",
        "# choose one function to try\n",
        "f = lambda x: 0.5 * x ** 2 # Convex\n",
        "# f = lambda x: np.cos(np.pi * x)  # Nonconvex\n",
        "# f = lambda x: -0.5 * x ** 4  # Nonconvex\n",
        "\n",
        "filenames=[]\n",
        "for lamda in np.arange(0, 1, 0.02):\n",
        "  # LHS\n",
        "  tmp_x = lamda*x[0] + (1-lamda)*x[-1]\n",
        "\n",
        "  # RHS\n",
        "  x_line, y_line = np.array([x[0], x[-1]]), np.array([lamda*f(x[0]), (1-lamda)*f(x[-1])])\n",
        "\n",
        "  # compute LHS and RHS\n",
        "  LHS = f(tmp_x)\n",
        "  RHS = lamda*f(x[0]) + (1-lamda)*f(x[-1])\n",
        "  if LHS > RHS:\n",
        "    print('At lamda %0.3f, it is concave' % lamda)\n",
        "    print('lhs %.5f rhs %.5f' % (LHS, RHS))\n",
        "\n",
        "  plt.figure()\n",
        "  # original graph\n",
        "  plt.plot(x, f(x), label='f(x)')\n",
        "  # plot RHS\n",
        "  plt.plot(x_line, y_line, label='%0.3f' % lamda)\n",
        "  # plot LHS\n",
        "  plt.scatter(tmp_x, f(tmp_x))\n",
        "  #title, legennd\n",
        "  plt.title('lhs %.3f rhs %.3f' % (LHS, RHS))\n",
        "  plt.legend()\n",
        "  plt.savefig('lamda %0.3f.png' % lamda)\n",
        "  # plt.close()\n",
        "  filenames.append('lamda %0.3f.png' % lamda)\n",
        "\n",
        "# Build GIF\n",
        "with imageio.get_writer('mygif.gif', mode='I') as writer:\n",
        "    for filename in filenames:\n",
        "        image = imageio.imread(filename)\n",
        "        writer.append_data(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9Wuu9o0pGmg"
      },
      "source": [
        "# Understand how learning rate affects your SGD optimization\n",
        "\n",
        "We will train a neural network for a pretty simple task, i.e. calculating the exclusive-or (XOR) of two input. \n",
        "\n",
        "<br> \n",
        "<img src=\"https://raw.githubusercontent.com/shiernee/Advanced_ML/main/Week3/XOR.jpg\" width=\"512\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr2IABWKoGHk"
      },
      "source": [
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PpJZLhyndlE",
        "outputId": "b780d629-bcb3-46dd-82f4-b50699ec8d0a"
      },
      "source": [
        "# generate a function for XOR\n",
        "x1 = random.randint(0, 1)\n",
        "x2 = random.randint(0, 1)\n",
        "yy = 0 if (x1 == x2) else 1\n",
        "\n",
        "print('x1:', x1)\n",
        "print('x2:',x2)\n",
        "print('yy:',yy)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sucvJe_fntPU",
        "outputId": "3dff8ccc-9361-430c-f2a8-5fe510e90b2f"
      },
      "source": [
        "x1 = random.randint(0, 1)\n",
        "x2 = random.randint(0, 1)\n",
        "yy = 0 if (x1 == x2) else 1\n",
        "\n",
        "# centered at zero\n",
        "x1 = 2. * (x1 - 0.5)\n",
        "x2 = 2. * (x2 - 0.5)\n",
        "yy = 2. * (yy - 0.5)\n",
        "\n",
        "print('x1:', x1)\n",
        "print('x2:',x2)\n",
        "print('yy:',yy)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1: -1.0\n",
            "x2: 1.0\n",
            "yy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEbi2kK4n9Ci",
        "outputId": "75c1ede3-5545-40bb-80bc-01fecf04192f"
      },
      "source": [
        "x1 = random.randint(0, 1)\n",
        "x2 = random.randint(0, 1)\n",
        "yy = 0 if (x1 == x2) else 1\n",
        "\n",
        "# centered at zero\n",
        "x1 = 2. * (x1 - 0.5)\n",
        "x2 = 2. * (x2 - 0.5)\n",
        "yy = 2. * (yy - 0.5)\n",
        "\n",
        "# add noise\n",
        "x1 += 0.1 * random.random()\n",
        "x2 += 0.1 * random.random()\n",
        "yy += 0.1 * random.random()\n",
        "\n",
        "print('x1:', x1)\n",
        "print('x2:',x2)\n",
        "print('yy:',yy)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1: -0.9574273128896689\n",
            "x2: -0.9030175180760335\n",
            "yy: -0.9475814083656239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RiENlDbpGS3"
      },
      "source": [
        "# make it into function \n",
        "def make_data():\n",
        "    x1 = random.randint(0, 1)\n",
        "    x2 = random.randint(0, 1)\n",
        "    yy = 0 if (x1 == x2) else 1\n",
        " \n",
        "    # centered at zero\n",
        "    x1 = 2. * (x1 - 0.5)\n",
        "    x2 = 2. * (x2 - 0.5)\n",
        "    yy = 2. * (yy - 0.5)\n",
        " \n",
        "    # add noise\n",
        "    x1 += 0.1 * random.random()\n",
        "    x2 += 0.1 * random.random()\n",
        "    yy += 0.1 * random.random()\n",
        " \n",
        "    return [x1, x2, ], yy\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRS81-dioL4n"
      },
      "source": [
        "# create batch samples\n",
        "batch_size = 10\n",
        "def make_batch():\n",
        "    data = [make_data() for ii in range(batch_size)]\n",
        "    labels = [label for xx, label in data]\n",
        "    data = [xx for xx, label in data]\n",
        "    return np.array(data, dtype='float32'), np.array(labels, dtype='float32')\n",
        " \n",
        "print(make_batch())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB7kASWVoZJi"
      },
      "source": [
        "# generate  500 train and 50 test data \n",
        "train_data = [make_batch() for ii in range(500)]\n",
        "test_data = [make_batch() for ii in range(50)]\n"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huxK2x7WpUGw"
      },
      "source": [
        "# import torch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        " "
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMj_0PO0ojry"
      },
      "source": [
        "## Define our neural network class\n",
        "torch.manual_seed(42)\n",
        " \n",
        "class NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NN, self).__init__()\n",
        " \n",
        "        self.dense1 = nn.Linear(2, 2)\n",
        "        self.dense2 = nn.Linear(2, 1)\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = F.tanh(self.dense1(x))\n",
        "        x = self.dense2(x)\n",
        "        return torch.squeeze(x)\n",
        " \n"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqLyWCLOtrJS"
      },
      "source": [
        "# initialize our network\n",
        "model = NN()\n",
        "\n",
        "## optimizer = stochastic gradient descent\n",
        "optimizer = optim.SGD(model.parameters(), lr)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Furjt7ZppdxA"
      },
      "source": [
        "## train and test functions\n",
        " \n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_data):\n",
        "        data, target = Variable(torch.from_numpy(data)), Variable(torch.from_numpy(target))\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.mse_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} {}\\tLoss: {:.4f}'.format(epoch, batch_idx * len(data), loss.item()))\n",
        " \n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_data:\n",
        "        data, target = Variable(torch.from_numpy(data), volatile=True), Variable(torch.from_numpy(target))\n",
        "        output = model(data)\n",
        "        test_loss += F.mse_loss(output, target)\n",
        "        correct += (np.around(output.data.numpy()) == np.around(target.data.numpy())).sum()\n",
        " \n",
        "    test_loss /= len(test_data)\n",
        "    test_loss = test_loss.item()\n",
        " \n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, batch_size * len(test_data), 100. * correct / (batch_size * len(test_data))) )\n",
        " "
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ4c_vUzphKy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d93a88-ce66-41e1-f823-1de85b60c26e"
      },
      "source": [
        "## run experiment \n",
        "nepochs = 1000\n",
        "lr = 0.001\n",
        "\n",
        "print('lr=', lr)\n",
        "for epoch in range(1, nepochs + 1):\n",
        "    train(epoch)\n",
        "    print('---------------------------------------------')\n",
        "    test()\n",
        " \n",
        " # everytime rerun this cell, please re initialize your network, and re run the train test function "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 497 1000\tLoss: 0.4051\n",
            "Train Epoch: 497 2000\tLoss: 0.3741\n",
            "Train Epoch: 497 3000\tLoss: 0.6958\n",
            "Train Epoch: 497 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 498 0\tLoss: 0.2817\n",
            "Train Epoch: 498 1000\tLoss: 0.4051\n",
            "Train Epoch: 498 2000\tLoss: 0.3741\n",
            "Train Epoch: 498 3000\tLoss: 0.6958\n",
            "Train Epoch: 498 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 499 0\tLoss: 0.2817\n",
            "Train Epoch: 499 1000\tLoss: 0.4051\n",
            "Train Epoch: 499 2000\tLoss: 0.3741\n",
            "Train Epoch: 499 3000\tLoss: 0.6958\n",
            "Train Epoch: 499 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 500 0\tLoss: 0.2817\n",
            "Train Epoch: 500 1000\tLoss: 0.4051\n",
            "Train Epoch: 500 2000\tLoss: 0.3741\n",
            "Train Epoch: 500 3000\tLoss: 0.6958\n",
            "Train Epoch: 500 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 501 0\tLoss: 0.2817\n",
            "Train Epoch: 501 1000\tLoss: 0.4051\n",
            "Train Epoch: 501 2000\tLoss: 0.3741\n",
            "Train Epoch: 501 3000\tLoss: 0.6958\n",
            "Train Epoch: 501 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 502 0\tLoss: 0.2817\n",
            "Train Epoch: 502 1000\tLoss: 0.4051\n",
            "Train Epoch: 502 2000\tLoss: 0.3741\n",
            "Train Epoch: 502 3000\tLoss: 0.6958\n",
            "Train Epoch: 502 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 503 0\tLoss: 0.2817\n",
            "Train Epoch: 503 1000\tLoss: 0.4051\n",
            "Train Epoch: 503 2000\tLoss: 0.3741\n",
            "Train Epoch: 503 3000\tLoss: 0.6958\n",
            "Train Epoch: 503 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 504 0\tLoss: 0.2817\n",
            "Train Epoch: 504 1000\tLoss: 0.4051\n",
            "Train Epoch: 504 2000\tLoss: 0.3741\n",
            "Train Epoch: 504 3000\tLoss: 0.6958\n",
            "Train Epoch: 504 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 505 0\tLoss: 0.2817\n",
            "Train Epoch: 505 1000\tLoss: 0.4051\n",
            "Train Epoch: 505 2000\tLoss: 0.3741\n",
            "Train Epoch: 505 3000\tLoss: 0.6958\n",
            "Train Epoch: 505 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 506 0\tLoss: 0.2817\n",
            "Train Epoch: 506 1000\tLoss: 0.4051\n",
            "Train Epoch: 506 2000\tLoss: 0.3741\n",
            "Train Epoch: 506 3000\tLoss: 0.6958\n",
            "Train Epoch: 506 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 507 0\tLoss: 0.2817\n",
            "Train Epoch: 507 1000\tLoss: 0.4051\n",
            "Train Epoch: 507 2000\tLoss: 0.3741\n",
            "Train Epoch: 507 3000\tLoss: 0.6958\n",
            "Train Epoch: 507 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 508 0\tLoss: 0.2817\n",
            "Train Epoch: 508 1000\tLoss: 0.4051\n",
            "Train Epoch: 508 2000\tLoss: 0.3741\n",
            "Train Epoch: 508 3000\tLoss: 0.6958\n",
            "Train Epoch: 508 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 509 0\tLoss: 0.2817\n",
            "Train Epoch: 509 1000\tLoss: 0.4051\n",
            "Train Epoch: 509 2000\tLoss: 0.3741\n",
            "Train Epoch: 509 3000\tLoss: 0.6958\n",
            "Train Epoch: 509 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 510 0\tLoss: 0.2817\n",
            "Train Epoch: 510 1000\tLoss: 0.4051\n",
            "Train Epoch: 510 2000\tLoss: 0.3741\n",
            "Train Epoch: 510 3000\tLoss: 0.6958\n",
            "Train Epoch: 510 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 511 0\tLoss: 0.2817\n",
            "Train Epoch: 511 1000\tLoss: 0.4051\n",
            "Train Epoch: 511 2000\tLoss: 0.3741\n",
            "Train Epoch: 511 3000\tLoss: 0.6958\n",
            "Train Epoch: 511 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 512 0\tLoss: 0.2817\n",
            "Train Epoch: 512 1000\tLoss: 0.4051\n",
            "Train Epoch: 512 2000\tLoss: 0.3741\n",
            "Train Epoch: 512 3000\tLoss: 0.6958\n",
            "Train Epoch: 512 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 513 0\tLoss: 0.2817\n",
            "Train Epoch: 513 1000\tLoss: 0.4051\n",
            "Train Epoch: 513 2000\tLoss: 0.3741\n",
            "Train Epoch: 513 3000\tLoss: 0.6958\n",
            "Train Epoch: 513 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 514 0\tLoss: 0.2817\n",
            "Train Epoch: 514 1000\tLoss: 0.4051\n",
            "Train Epoch: 514 2000\tLoss: 0.3741\n",
            "Train Epoch: 514 3000\tLoss: 0.6958\n",
            "Train Epoch: 514 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 515 0\tLoss: 0.2817\n",
            "Train Epoch: 515 1000\tLoss: 0.4051\n",
            "Train Epoch: 515 2000\tLoss: 0.3741\n",
            "Train Epoch: 515 3000\tLoss: 0.6958\n",
            "Train Epoch: 515 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 516 0\tLoss: 0.2817\n",
            "Train Epoch: 516 1000\tLoss: 0.4051\n",
            "Train Epoch: 516 2000\tLoss: 0.3741\n",
            "Train Epoch: 516 3000\tLoss: 0.6958\n",
            "Train Epoch: 516 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 517 0\tLoss: 0.2817\n",
            "Train Epoch: 517 1000\tLoss: 0.4051\n",
            "Train Epoch: 517 2000\tLoss: 0.3741\n",
            "Train Epoch: 517 3000\tLoss: 0.6958\n",
            "Train Epoch: 517 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 518 0\tLoss: 0.2817\n",
            "Train Epoch: 518 1000\tLoss: 0.4051\n",
            "Train Epoch: 518 2000\tLoss: 0.3741\n",
            "Train Epoch: 518 3000\tLoss: 0.6958\n",
            "Train Epoch: 518 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 519 0\tLoss: 0.2817\n",
            "Train Epoch: 519 1000\tLoss: 0.4051\n",
            "Train Epoch: 519 2000\tLoss: 0.3741\n",
            "Train Epoch: 519 3000\tLoss: 0.6958\n",
            "Train Epoch: 519 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 520 0\tLoss: 0.2817\n",
            "Train Epoch: 520 1000\tLoss: 0.4051\n",
            "Train Epoch: 520 2000\tLoss: 0.3741\n",
            "Train Epoch: 520 3000\tLoss: 0.6958\n",
            "Train Epoch: 520 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 521 0\tLoss: 0.2817\n",
            "Train Epoch: 521 1000\tLoss: 0.4051\n",
            "Train Epoch: 521 2000\tLoss: 0.3741\n",
            "Train Epoch: 521 3000\tLoss: 0.6958\n",
            "Train Epoch: 521 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 522 0\tLoss: 0.2817\n",
            "Train Epoch: 522 1000\tLoss: 0.4051\n",
            "Train Epoch: 522 2000\tLoss: 0.3741\n",
            "Train Epoch: 522 3000\tLoss: 0.6958\n",
            "Train Epoch: 522 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 523 0\tLoss: 0.2817\n",
            "Train Epoch: 523 1000\tLoss: 0.4051\n",
            "Train Epoch: 523 2000\tLoss: 0.3741\n",
            "Train Epoch: 523 3000\tLoss: 0.6958\n",
            "Train Epoch: 523 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 524 0\tLoss: 0.2817\n",
            "Train Epoch: 524 1000\tLoss: 0.4051\n",
            "Train Epoch: 524 2000\tLoss: 0.3741\n",
            "Train Epoch: 524 3000\tLoss: 0.6958\n",
            "Train Epoch: 524 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 525 0\tLoss: 0.2817\n",
            "Train Epoch: 525 1000\tLoss: 0.4051\n",
            "Train Epoch: 525 2000\tLoss: 0.3741\n",
            "Train Epoch: 525 3000\tLoss: 0.6958\n",
            "Train Epoch: 525 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 526 0\tLoss: 0.2817\n",
            "Train Epoch: 526 1000\tLoss: 0.4051\n",
            "Train Epoch: 526 2000\tLoss: 0.3741\n",
            "Train Epoch: 526 3000\tLoss: 0.6958\n",
            "Train Epoch: 526 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 527 0\tLoss: 0.2817\n",
            "Train Epoch: 527 1000\tLoss: 0.4051\n",
            "Train Epoch: 527 2000\tLoss: 0.3741\n",
            "Train Epoch: 527 3000\tLoss: 0.6958\n",
            "Train Epoch: 527 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 528 0\tLoss: 0.2817\n",
            "Train Epoch: 528 1000\tLoss: 0.4051\n",
            "Train Epoch: 528 2000\tLoss: 0.3741\n",
            "Train Epoch: 528 3000\tLoss: 0.6958\n",
            "Train Epoch: 528 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 529 0\tLoss: 0.2817\n",
            "Train Epoch: 529 1000\tLoss: 0.4051\n",
            "Train Epoch: 529 2000\tLoss: 0.3741\n",
            "Train Epoch: 529 3000\tLoss: 0.6958\n",
            "Train Epoch: 529 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 530 0\tLoss: 0.2817\n",
            "Train Epoch: 530 1000\tLoss: 0.4051\n",
            "Train Epoch: 530 2000\tLoss: 0.3741\n",
            "Train Epoch: 530 3000\tLoss: 0.6958\n",
            "Train Epoch: 530 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 531 0\tLoss: 0.2817\n",
            "Train Epoch: 531 1000\tLoss: 0.4051\n",
            "Train Epoch: 531 2000\tLoss: 0.3741\n",
            "Train Epoch: 531 3000\tLoss: 0.6958\n",
            "Train Epoch: 531 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 532 0\tLoss: 0.2817\n",
            "Train Epoch: 532 1000\tLoss: 0.4051\n",
            "Train Epoch: 532 2000\tLoss: 0.3741\n",
            "Train Epoch: 532 3000\tLoss: 0.6958\n",
            "Train Epoch: 532 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 533 0\tLoss: 0.2817\n",
            "Train Epoch: 533 1000\tLoss: 0.4051\n",
            "Train Epoch: 533 2000\tLoss: 0.3741\n",
            "Train Epoch: 533 3000\tLoss: 0.6958\n",
            "Train Epoch: 533 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 534 0\tLoss: 0.2817\n",
            "Train Epoch: 534 1000\tLoss: 0.4051\n",
            "Train Epoch: 534 2000\tLoss: 0.3741\n",
            "Train Epoch: 534 3000\tLoss: 0.6958\n",
            "Train Epoch: 534 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 535 0\tLoss: 0.2817\n",
            "Train Epoch: 535 1000\tLoss: 0.4051\n",
            "Train Epoch: 535 2000\tLoss: 0.3741\n",
            "Train Epoch: 535 3000\tLoss: 0.6958\n",
            "Train Epoch: 535 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 536 0\tLoss: 0.2817\n",
            "Train Epoch: 536 1000\tLoss: 0.4051\n",
            "Train Epoch: 536 2000\tLoss: 0.3741\n",
            "Train Epoch: 536 3000\tLoss: 0.6958\n",
            "Train Epoch: 536 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 537 0\tLoss: 0.2817\n",
            "Train Epoch: 537 1000\tLoss: 0.4051\n",
            "Train Epoch: 537 2000\tLoss: 0.3741\n",
            "Train Epoch: 537 3000\tLoss: 0.6958\n",
            "Train Epoch: 537 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 538 0\tLoss: 0.2817\n",
            "Train Epoch: 538 1000\tLoss: 0.4051\n",
            "Train Epoch: 538 2000\tLoss: 0.3741\n",
            "Train Epoch: 538 3000\tLoss: 0.6958\n",
            "Train Epoch: 538 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 539 0\tLoss: 0.2817\n",
            "Train Epoch: 539 1000\tLoss: 0.4051\n",
            "Train Epoch: 539 2000\tLoss: 0.3741\n",
            "Train Epoch: 539 3000\tLoss: 0.6958\n",
            "Train Epoch: 539 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 540 0\tLoss: 0.2817\n",
            "Train Epoch: 540 1000\tLoss: 0.4051\n",
            "Train Epoch: 540 2000\tLoss: 0.3741\n",
            "Train Epoch: 540 3000\tLoss: 0.6958\n",
            "Train Epoch: 540 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 541 0\tLoss: 0.2817\n",
            "Train Epoch: 541 1000\tLoss: 0.4051\n",
            "Train Epoch: 541 2000\tLoss: 0.3741\n",
            "Train Epoch: 541 3000\tLoss: 0.6958\n",
            "Train Epoch: 541 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 542 0\tLoss: 0.2817\n",
            "Train Epoch: 542 1000\tLoss: 0.4051\n",
            "Train Epoch: 542 2000\tLoss: 0.3741\n",
            "Train Epoch: 542 3000\tLoss: 0.6958\n",
            "Train Epoch: 542 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 543 0\tLoss: 0.2817\n",
            "Train Epoch: 543 1000\tLoss: 0.4051\n",
            "Train Epoch: 543 2000\tLoss: 0.3741\n",
            "Train Epoch: 543 3000\tLoss: 0.6958\n",
            "Train Epoch: 543 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 544 0\tLoss: 0.2817\n",
            "Train Epoch: 544 1000\tLoss: 0.4051\n",
            "Train Epoch: 544 2000\tLoss: 0.3741\n",
            "Train Epoch: 544 3000\tLoss: 0.6958\n",
            "Train Epoch: 544 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 545 0\tLoss: 0.2817\n",
            "Train Epoch: 545 1000\tLoss: 0.4051\n",
            "Train Epoch: 545 2000\tLoss: 0.3741\n",
            "Train Epoch: 545 3000\tLoss: 0.6958\n",
            "Train Epoch: 545 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 546 0\tLoss: 0.2817\n",
            "Train Epoch: 546 1000\tLoss: 0.4051\n",
            "Train Epoch: 546 2000\tLoss: 0.3741\n",
            "Train Epoch: 546 3000\tLoss: 0.6958\n",
            "Train Epoch: 546 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 547 0\tLoss: 0.2817\n",
            "Train Epoch: 547 1000\tLoss: 0.4051\n",
            "Train Epoch: 547 2000\tLoss: 0.3741\n",
            "Train Epoch: 547 3000\tLoss: 0.6958\n",
            "Train Epoch: 547 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 548 0\tLoss: 0.2817\n",
            "Train Epoch: 548 1000\tLoss: 0.4051\n",
            "Train Epoch: 548 2000\tLoss: 0.3741\n",
            "Train Epoch: 548 3000\tLoss: 0.6958\n",
            "Train Epoch: 548 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 549 0\tLoss: 0.2817\n",
            "Train Epoch: 549 1000\tLoss: 0.4051\n",
            "Train Epoch: 549 2000\tLoss: 0.3741\n",
            "Train Epoch: 549 3000\tLoss: 0.6958\n",
            "Train Epoch: 549 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 550 0\tLoss: 0.2817\n",
            "Train Epoch: 550 1000\tLoss: 0.4051\n",
            "Train Epoch: 550 2000\tLoss: 0.3741\n",
            "Train Epoch: 550 3000\tLoss: 0.6958\n",
            "Train Epoch: 550 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 551 0\tLoss: 0.2817\n",
            "Train Epoch: 551 1000\tLoss: 0.4051\n",
            "Train Epoch: 551 2000\tLoss: 0.3741\n",
            "Train Epoch: 551 3000\tLoss: 0.6958\n",
            "Train Epoch: 551 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 552 0\tLoss: 0.2817\n",
            "Train Epoch: 552 1000\tLoss: 0.4051\n",
            "Train Epoch: 552 2000\tLoss: 0.3741\n",
            "Train Epoch: 552 3000\tLoss: 0.6958\n",
            "Train Epoch: 552 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 553 0\tLoss: 0.2817\n",
            "Train Epoch: 553 1000\tLoss: 0.4051\n",
            "Train Epoch: 553 2000\tLoss: 0.3741\n",
            "Train Epoch: 553 3000\tLoss: 0.6958\n",
            "Train Epoch: 553 4000\tLoss: 0.5083\n",
            "---------------------------------------------\n",
            "\n",
            "Test set: Average loss: 0.5481, Accuracy: 232/500 (46.40%)\n",
            "\n",
            "Train Epoch: 554 0\tLoss: 0.2817\n",
            "Train Epoch: 554 1000\tLoss: 0.4051\n",
            "Train Epoch: 554 2000\tLoss: 0.3741\n",
            "Train Epoch: 554 3000\tLoss: 0.6958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM3QoS2buZe7"
      },
      "source": [
        "## Exercise 3 (6%) \n",
        "For this experiment, try the following learning rate (lr=0.0001, 0.001, 0.01, 0.1). What do you observed? <br><br>\n",
        "For example, at lr=0.001, test acc reach 100% at epoch xx... At lr=0.001, test acc reach 100% at epoch xx. As lr increases / decreases, what happen?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH5tn9G1ugD1"
      },
      "source": [
        "### Your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1d47bLhDMlw"
      },
      "source": [
        "# Submission Instructions\n",
        "Once you are finished, follow these steps:\n",
        "\n",
        "Restart the kernel and re-run this notebook from beginning to end by going to Kernel > Restart Kernel and Run All Cells.\n",
        "If this process stops halfway through, that means there was an error. Correct the error and repeat Step 1 until the notebook runs from beginning to end.\n",
        "Double check that there is a number next to each code cell and that these numbers are in order.\n",
        "Then, submit your lab as follows:\n",
        "\n",
        "Go to File > Print > Save as PDF.\n",
        "Double check that the entire notebook, from beginning to end, is in this PDF file. Make sure Solution for Exercise 5 are in for marks. \n",
        "Upload the PDF to Spectrum. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FBd4KLZwyVB"
      },
      "source": [
        "# Acknowledgement\n",
        "\n",
        "Some of the works are inspired from \n",
        "1. Effect of learning rate on AI model = https://www.commonlounge.com/discussion/5076b2cfb2364594ba608fca3ac606bb"
      ]
    }
  ]
}